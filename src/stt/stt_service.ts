import { Injectable, Logger } from '@nestjs/common';
import { GoogleSpeechProvider } from './providers/google-speech';
import { AudioProcessorUtil } from './utils/audio-processer';
import { TextProcessorUtil } from './utils/text_processor';
import { TranscriptionResult, STTResult } from './entities/transcription';
import { SpeechPatternsUtil } from './utils/speech-patterms';
import { PynoteService } from './providers/pynote.service';

@Injectable()
export class STTService {
    private readonly logger = new Logger(STTService.name);

    constructor(
        private readonly googleSpeechProvider: GoogleSpeechProvider,
        private readonly pynoteService: PynoteService,
    ) {}

    async transcribeAudioBuffer(
        audioBuffer: Buffer,
        mimeType = 'audio/mp4',
        sessionStartTimeOffset = 0,
        gcsUrl?: string,
        usePynoteDiarization = false,
    ): Promise<STTResult> {
        if (usePynoteDiarization && gcsUrl) {
            return await this.transcribeAudioFromGcs(
                gcsUrl,
                mimeType,
                sessionStartTimeOffset,
                true,
            );
        }

        // ê¸°ì¡´ ë°©ì‹
        const base64Data = audioBuffer.toString('base64');
        const audioData = this.prepareAudioData(base64Data, gcsUrl);
        const config = this.createAudioConfig(mimeType);
        const result = await this.googleSpeechProvider.transcribe(audioData, config, gcsUrl);

        return this.adjustTimings(result, sessionStartTimeOffset);
    }

    private async transcribeWithPynoteDiarizationFromGcs(
        gcsUrl: string,
        mimeType: string,
        sessionStartTimeOffset: number,
        canvasId: string,
        mentorIdx?: number,
        menteeIdx?: number,
    ): Promise<STTResult> {
        try {
            this.logger.log('ï¿½ï¿½ pynote GCS ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¦¬ + ì„¸ê·¸ë¨¼íŠ¸ë³„ STT ì‹œì‘');

            // 1. pynoteì—ì„œ GCS URLë¡œ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¦¬
            const segmentResult = await this.pynoteService.getSegmentsFromGcs(
                gcsUrl,
                canvasId, // ì„ì‹œ ìº”ë²„ìŠ¤ ID
                mentorIdx || 1,
                menteeIdx || 2,
                sessionStartTimeOffset,
            );

            if (!segmentResult.success || segmentResult.segments.length === 0) {
                throw new Error('pynote ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¦¬ ì‹¤íŒ¨');
            }

            this.logger.log(
                `âœ… pynote ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¦¬ ì™„ë£Œ: ${segmentResult.segments.length}ê°œ ì„¸ê·¸ë¨¼íŠ¸`,
            );

            // 2. ê° ì„¸ê·¸ë¨¼íŠ¸ ë²„í¼ë¡œ STT ì‹¤í–‰
            const allSpeakers: Array<{
                text_Content: string;
                startTime: number;
                endTime: number;
                speakerTag: number;
                confidence?: number;
            }> = [];

            for (let i = 0; i < segmentResult.segments.length; i++) {
                const segment = segmentResult.segments[i];
                this.logger.log(
                    `ï¿½ï¿½ ì„¸ê·¸ë¨¼íŠ¸ ${i + 1}/${segmentResult.segments.length} STT ì²˜ë¦¬ ì‹œì‘`,
                );

                try {
                    const audioBuffer = Buffer.from(segment.audioBuffer, 'base64');

                    // Google Speechë¡œ ì„¸ê·¸ë¨¼íŠ¸ STT ì‹¤í–‰
                    const base64Data = audioBuffer.toString('base64');
                    const audioData = this.prepareAudioData(base64Data, '');
                    const config = this.createAudioConfigWithoutDiarization(mimeType);
                    const sttResult = await this.googleSpeechProvider.transcribe(audioData, config);

                    // ì„¸ê·¸ë¨¼íŠ¸ ê²°ê³¼ë¥¼ ì „ì²´ ê²°ê³¼ì— ì¶”ê°€
                    if (sttResult.speakers && sttResult.speakers.length > 0) {
                        for (const speaker of sttResult.speakers) {
                            allSpeakers.push({
                                ...speaker,
                                speakerTag: segment.speakerTag,
                                startTime:
                                    sessionStartTimeOffset + segment.startTime + speaker.startTime,
                                endTime:
                                    sessionStartTimeOffset + segment.startTime + speaker.endTime,
                            });
                        }
                    } else if (sttResult.transcript) {
                        // STT ê²°ê³¼ê°€ ìˆì§€ë§Œ speakersê°€ ì—†ëŠ” ê²½ìš°
                        allSpeakers.push({
                            text_Content: sttResult.transcript,
                            speakerTag: segment.speakerTag,
                            startTime: sessionStartTimeOffset + segment.startTime,
                            endTime: sessionStartTimeOffset + segment.endTime,
                            confidence: sttResult.confidence || 0.9,
                        });
                    }

                    this.logger.log(`âœ… ì„¸ê·¸ë¨¼íŠ¸ ${i + 1} STT ì™„ë£Œ: "${sttResult.transcript}"`);
                } catch (segmentError) {
                    this.logger.error(
                        `âŒ ì„¸ê·¸ë¨¼íŠ¸ ${i + 1} STT ì‹¤íŒ¨: ${segmentError instanceof Error ? segmentError.message : String(segmentError)}`,
                    );
                    // ì‹¤íŒ¨í•œ ì„¸ê·¸ë¨¼íŠ¸ëŠ” ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰
                }
            }

            this.logger.log(
                `âœ… pynote ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¦¬ + STT ì²˜ë¦¬ ì™„ë£Œ: ${allSpeakers.length}ê°œ ì„¸ê·¸ë¨¼íŠ¸`,
            );

            return {
                transcript: allSpeakers.map((s) => s.text_Content).join(' '),
                confidence: 0.9,
                speakers: allSpeakers,
            };
        } catch (error: unknown) {
            this.logger.error(
                `pynote GCS ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¦¬ + STT ì²˜ë¦¬ ì‹¤íŒ¨: ${error instanceof Error ? error.message : String(error)}`,
            );

            // fallback to Google Speech
            return await this.transcribeWithGoogleSpeech(gcsUrl, mimeType);
        }
    }

    // ï¿½ï¿½ Google Speech ì§ì ‘ ì‚¬ìš© (fallbackìš©)
    private async transcribeWithGoogleSpeech(gcsUrl: string, mimeType: string): Promise<STTResult> {
        try {
            this.logger.log('ğŸ”„ Google Speech ì§ì ‘ ì‚¬ìš© (fallback)');

            const audioData = this.prepareAudioData('', gcsUrl);
            const config = this.createAudioConfig(mimeType);
            const result = await this.googleSpeechProvider.transcribe(audioData, config, gcsUrl);

            return this.adjustTimings(result, 0);
        } catch (error) {
            this.logger.error(
                `Google Speech fallback ì‹¤íŒ¨: ${error instanceof Error ? error.message : String(error)}`,
            );
            throw error;
        }
    }

    async transcribeAudioFromGcs(
        gcsUrl: string,
        mimeType = 'audio/mp4',
        sessionStartTimeOffset = 0,
        usePynoteDiarization = true,
        canvasId?: string,
        mentorIdx?: number,
        menteeIdx?: number,
    ): Promise<STTResult> {
        if (usePynoteDiarization) {
            return await this.transcribeWithPynoteDiarizationFromGcs(
                gcsUrl,
                mimeType,
                sessionStartTimeOffset,
                canvasId || 'resume-room',
                mentorIdx,
                menteeIdx,
            );
        }

        // ê¸°ì¡´ ë°©ì‹ (GCS URL ì‚¬ìš©)
        const audioData = this.prepareAudioData('', gcsUrl);
        const config = this.createAudioConfig(mimeType);
        const result = await this.googleSpeechProvider.transcribe(audioData, config, gcsUrl);

        return this.adjustTimings(result, sessionStartTimeOffset);
    }

    // ğŸ†• í™”ìë¶„ë¦¬ ë¹„í™œì„±í™”ëœ ì„¤ì • ìƒì„±
    private createAudioConfigWithoutDiarization(mimeType: string) {
        const baseConfig = this.createAudioConfig(mimeType);
        return {
            ...baseConfig,
            enableSpeakerDiarization: false, // í™”ìë¶„ë¦¬ ë¹„í™œì„±í™”
            diarizationSpeakerCount: 0,
            enableWordTimeOffsets: true, // â† ì´ ì¤„ ì¶”ê°€!
        };
    }

    private prepareAudioData(base64Data: string, gcsUrl?: string): string {
        if (gcsUrl) {
            return AudioProcessorUtil.convertToGcsUri(gcsUrl);
        }
        // ì›ë˜ëŠ” ë‹¨ìˆœíˆ base64 ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜í–ˆì„ ê²ƒ
        return base64Data; // â† ì´ì œ ì›ë˜ëŒ€ë¡œ ë‹¨ìˆœíˆ base64 ë°˜í™˜
    }

    private createAudioConfig(mimeType: string) {
        // MP4/M4A íŒŒì¼ì˜ ê²½ìš° ë‹¤ë¥¸ ì„¤ì • ì‚¬ìš©
        if (mimeType.includes('mp4') || mimeType.includes('m4a')) {
            return {
                encoding: 'MP3', // MP3 ì¸ì½”ë”© ì‚¬ìš©
                sampleRate: 44100,
                languageCode: 'ko-KR',
                enableSpeakerDiarization: true,
                diarizationSpeakerCount: 2,
                enableAutomaticPunctuation: false,
                maxAlternatives: 1,
                speechContexts: [], // ë¹ˆ ë°°ì—´ë¡œ ì„¤ì •
            };
        }

        // ë‹¤ë¥¸ í¬ë§·ë“¤
        let encoding: 'LINEAR16' | 'MP3' | 'WEBM_OPUS' | 'FLAC' = 'LINEAR16';
        if (mimeType.includes('mp3')) {
            encoding = 'MP3';
        } else if (mimeType.includes('webm') || mimeType.includes('opus')) {
            encoding = 'WEBM_OPUS';
        } else if (mimeType.includes('flac')) {
            encoding = 'FLAC';
        }

        return {
            encoding,
            sampleRate: 44100,
            languageCode: 'ko-KR',
            enableSpeakerDiarization: true,
            diarizationSpeakerCount: 2,
            enableAutomaticPunctuation: true,
            minSpeakerCount: 2,
            maxSpeakerCount: 2,
            enableWordTimeOffsets: true, // ğŸ†• ì¶”ê°€
            useEnhanced: true,
            maxAlternatives: 1,
            speechContexts: SpeechPatternsUtil.SPEECH_CONTEXTS,
        };
    }

    private adjustTimings(result: TranscriptionResult, sessionStartTimeOffset: number): STTResult {
        // TranscriptionResultë¥¼ STTResultë¡œ ë³€í™˜
        let speakers: Array<{
            text_Content: string;
            startTime: number;
            endTime: number;
            speakerTag: number;
        }> =
            result.speakers?.map((speaker) => ({
                text_Content: speaker.text_Content,
                startTime: Math.round((speaker.startTime + sessionStartTimeOffset) * 10) / 10,
                endTime: Math.round((speaker.endTime + sessionStartTimeOffset) * 10) / 10,
                speakerTag: speaker.speakerTag,
            })) || [];

        // ì—‰ëš±í•œ ë‹¨ì–´ êµì • ë° ë¬¸ì¥ ê°œì„  ì ìš©
        speakers = TextProcessorUtil.processAndCorrectText(speakers);

        // ë¬¸ì¥ ì—°ê²°ì„± ê°œì„ 
        speakers = TextProcessorUtil.improveConversationFlow(speakers);

        const sttResult: STTResult = {
            transcript: result.transcript,
            confidence: result.confidence,
            speakers: speakers,
        };

        return sttResult;
    }
}
